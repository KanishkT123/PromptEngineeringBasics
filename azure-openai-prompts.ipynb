{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was auto-generated by GitHub Copilot Chat and is meant for initial setup only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt Engineering\n",
    "Prompt engineering is the process of designing and optimizing prompts for natural language processing tasks. It involves selecting the right prompts, tuning their parameters, and evaluating their performance. Prompt engineering is crucial for achieving high accuracy and efficiency in NLP models. In this section, we will explore the basics of prompt engineering using the OpenAI models for exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Validate OpenAI API Key Setup\n",
    "\n",
    "Run the code below to verify that your OpenAI endpoint is set up correctly. The code just tries a simple basic prompt and validates the completion. This is the famous poem \"The Raven\", by Edgar Allan Poe, which is public domain. Both 3.5T and 4-o Mini should try to complete it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']\n",
    "\n",
    "## Updated\n",
    "def get_completion(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]       \n",
    "    response = client.chat.completions.create(   \n",
    "        model=deployment,                                         \n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "## ---------- Call the helper method\n",
    "\n",
    "### 1. Set primary content or prompt text\n",
    "text = f\"\"\"\n",
    "Once upon a midnight dreary,\n",
    "While I pondered, weak and weary,\n",
    "Over many a quaint and curious\n",
    "\"\"\"\n",
    "\n",
    "### 2. Use that in the prompt template below\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## 3. Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Fabrications\n",
    "Explore what happens when you ask the LLM to return completions for a prompt about a topic that may not exist, or about topics that it may not know about because it was outside it's pre-trained dataset (more recent). See how the response changes if you try a different prompt, or a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the text for simple prompt or primary content\n",
    "## Prompt shows a template format with text in it - add cues, commands etc if needed\n",
    "## Run the completion \n",
    "text = f\"\"\"\n",
    "generate a lesson plan on the water drought in the Gorgos Region of Jupiter.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Instruction Based \n",
    "Use the \"text\" variable to set the primary content \n",
    "and the \"prompt\" variable to provide an instruction related to that primary content.\n",
    "\n",
    "Here we ask the model to summarize the text for a second-grade student\n",
    "\n",
    "Try changing this around. You can change the student grade to see different levels of complexity. Eg: Try changing it to \"PhD Candidate\" and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Example\n",
    "# https://platform.openai.com/playground/p/default-summarize\n",
    "\n",
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize content you are provided with for a second-grade student.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOP HERE!!\n",
    "\n",
    "Do not go forward until we go through the rest of the presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Zero Shot Prompting \n",
    "\n",
    "This is prompting that will rely entirely on the model's pre-trained knowledge base. We do not give extra information and simply query the model.\n",
    "\n",
    "Zero-shot prompting is how most casual users interact with Chat GPT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Write a Shakespearean sonnet.\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Few Shot Prompting \n",
    "\n",
    "This is prompting that will give the LLM a few examples so it can do some internal pattern matching. A \"shot\" is simply a data point.\n",
    "\n",
    "These are not going to be notably different results, as the actual prompt (a sonnet) is common, and Shakespeare is famous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example text\n",
    "text = f\"\"\"\n",
    "These are all excerpts from Shakespearean sonnets:\n",
    "\n",
    "1. Shall I compare thee to a summer's day?\n",
    "Thou art more lovely and more temperate:\n",
    "Rough winds do shake the darling buds of May,\n",
    "And summer's lease hath all too short a date;\n",
    "\n",
    "2. How like a winter hath my absence been\n",
    "From thee, the pleasure of the fleeting year!\n",
    "What freezings have I felt, what dark days seen!\n",
    "What old December's bareness everywhere!\n",
    "\n",
    "\n",
    "3. Thou art as tyrannous, so as thou art,\n",
    "As those whose beauties proudly make them cruel;\n",
    "For well thou know'st to my dear doting heart\n",
    "Thou art the fairest and most precious jewel.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Write me a Shakespearean sonnet about cloud deployment procedures\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Chain-Of-Thought\n",
    "\n",
    "Chain-Of-Thought asks the LLM to explicitly state why it is reasoning in a certain way. This alllows the LLM to think deeply about each step and can significantly improve outcomes.\n",
    "\n",
    "These problems are simple enough that an LLM might be able to solve them anyway! In my testing, it solved them about half the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the prompt\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Alice has 8 apples and 4 bananas, throws 2 bananas, gives John 3 apples, and eats 1 apple. How many apples does Alice have?\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "## Answer should be 4 apples, the LLM might get this wrong.\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "## Set the prompt\n",
    "text = f\"\"\"\n",
    "Here is an example of how to do problems like this.\n",
    "\n",
    "Problem: Alice has 8 apples and 4 bananas, throws 2 bananas, gives John 3 apples, and eats 1 apple. How many apples does Alice have?\n",
    "\n",
    "1. Start with the number of apples Alice has: 8\n",
    "2. She gives John 3 apples, so she has 8 - 3 = 5 apples\n",
    "3. She eats 1 apple, so she has 5 - 1 = 4 apples\n",
    "4. She has 4 apples left\n",
    "\n",
    "The answer is 4.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\n",
    "Now do this problem:\n",
    "Alice has 12 apples and 5 bananas, throws 4 bananas, gives John 6 apples, and eats 2 apples. How many apples does Alice have?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "## Answer should be 6 apples\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Least-To-Most\n",
    "\n",
    "This is a refinement technique. We will get the LLM to generate better output by asking it self-refine the answers it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Write a Web API in Python with an endpoint for pets and an endpoint for customers. Return only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{response}\n",
    "Suggest and implement 3 ways to improve the code.\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: ReACT\n",
    "\n",
    "This is a combination of prior techniques, allowing the LLM to reason through multiple logical steps. We will closely follow an example from the whitepaper, Yao et al 2022, \"ReAct: Synergizing Reasoning and Acting in Language Models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_answer = f\"\"\"\n",
    "Thought 1 I need to search Colorado orogeny, find the area that the eastern sector\n",
    "of the Colorado orogeny extends into, then find the elevation range of the\n",
    "area.\n",
    "Action 1 Search[Colorado orogeny]\n",
    "Observation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in\n",
    "Colorado and surrounding areas.\n",
    "Thought 2 It does not mention the eastern sector. So I need to look up eastern\n",
    "sector.\n",
    "Action 2 Lookup[eastern sector]\n",
    "Observation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called\n",
    "the Central Plains orogeny.\n",
    "Thought 3 The eastern sector of Colorado orogeny extends into the High Plains. So I\n",
    "need to search High Plains and find its elevation range.\n",
    "Action 3 Search[High Plains]\n",
    "Observation 3 High Plains refers to one of two distinct land regions\n",
    "Thought 4 I need to instead search High Plains (United States).\n",
    "Action 4 Search[High Plains (United States)]\n",
    "Observation 4 The High Plains are a subregion of the Great Plains. From east to west, the\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\n",
    "m).[3]\n",
    "Thought 5 High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer\n",
    "is 1,800 to 7,000 ft.\n",
    "Action 5 Finish[1,800 to 7,000 ft]\n",
    "\"\"\"\n",
    "\n",
    "question = \"What happened in the Great Martian War of 2061?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant responsible for \\\n",
    "         thinking through questions and logically giving an answer. \\\n",
    "         You need to state every thought and action you take when answering a question.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\"},\n",
    "        {\"role\": \"assistant\", \"content\": react_answer},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
